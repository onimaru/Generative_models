{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.datasets as dset\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.contrib.examples.util  # patches torchvision\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pyro.__version__.startswith('1.3.1')\n",
    "pyro.enable_validation(True)\n",
    "pyro.distributions.enable_validation(False)\n",
    "pyro.set_rng_seed(0)\n",
    "# Enable smoke test - run the notebook cells on CI.\n",
    "smoke_test = 'CI' in os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loading and batching MNIST dataset\n",
    "def setup_data_loaders(batch_size=128, use_cuda=False):\n",
    "    root = './data'\n",
    "    download = True\n",
    "    trans = transforms.ToTensor()\n",
    "    train_set = dset.MNIST(root=root, train=True, transform=trans,\n",
    "                           download=download)\n",
    "    test_set = dset.MNIST(root=root, train=False, transform=trans)\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': use_cuda}\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_set,\n",
    "        batch_size=batch_size, shuffle=False, **kwargs)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        # setup the two linear transformations used\n",
    "        self.fc1 = nn.Linear(z_dim, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, 784)\n",
    "        # setup the non-linearities\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, z):\n",
    "        # define the forward computation on the latent z\n",
    "        # first compute the hidden units\n",
    "        hidden = self.softplus(self.fc1(z))\n",
    "        # return the parameter for the output Bernoulli\n",
    "        # each is of size batch_size x 784\n",
    "        loc_img = self.sigmoid(self.fc21(hidden))\n",
    "        return loc_img\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        # setup the three linear transformations used\n",
    "        self.fc1 = nn.Linear(784, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, z_dim)\n",
    "        self.fc22 = nn.Linear(hidden_dim, z_dim)\n",
    "        # setup the non-linearities\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # define the forward computation on the image x\n",
    "        # first shape the mini-batch to have pixels in the rightmost dimension\n",
    "        x = x.reshape(-1, 784)\n",
    "        # then compute the hidden units\n",
    "        hidden = self.softplus(self.fc1(x))\n",
    "        # then return a mean vector and a (positive) square root covariance\n",
    "        # each of size batch_size x z_dim\n",
    "        z_loc = self.fc21(hidden)\n",
    "        z_scale = torch.exp(self.fc22(hidden))\n",
    "        return z_loc, z_scale\n",
    "    \n",
    "class VAE(nn.Module):\n",
    "    # by default our latent space is 50-dimensional\n",
    "    # and we use 400 hidden units\n",
    "    def __init__(self, z_dim=50, hidden_dim=400, use_cuda=False):\n",
    "        super().__init__()\n",
    "        # create the encoder and decoder networks\n",
    "        self.encoder = Encoder(z_dim, hidden_dim)\n",
    "        self.decoder = Decoder(z_dim, hidden_dim)\n",
    "\n",
    "        if use_cuda:\n",
    "            # calling cuda() here will put all the parameters of\n",
    "            # the encoder and decoder networks into gpu memory\n",
    "            self.cuda()\n",
    "        self.use_cuda = use_cuda\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "    # define the model p(x|z)p(z)\n",
    "    def model(self, x):\n",
    "        # register PyTorch module `decoder` with Pyro\n",
    "        pyro.module(\"decoder\", self.decoder)\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            # setup hyperparameters for prior p(z)\n",
    "            z_loc = x.new_zeros(torch.Size((x.shape[0], self.z_dim)))\n",
    "            z_scale = x.new_ones(torch.Size((x.shape[0], self.z_dim)))\n",
    "            # sample from prior (value will be sampled by guide when computing the ELBO)\n",
    "            z = pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))\n",
    "            # decode the latent code z\n",
    "            loc_img = self.decoder.forward(z)\n",
    "            # score against actual images\n",
    "            pyro.sample(\"obs\", dist.Bernoulli(loc_img).to_event(1), obs=x.reshape(-1, 784))\n",
    "\n",
    "    # define the guide (i.e. variational distribution) q(z|x)\n",
    "    def guide(self, x):\n",
    "        # register PyTorch module `encoder` with Pyro\n",
    "        pyro.module(\"encoder\", self.encoder)\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            # use the encoder to get the parameters used to define q(z|x)\n",
    "            z_loc, z_scale = self.encoder.forward(x)\n",
    "            # sample the latent code z\n",
    "            pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))\n",
    "\n",
    "    # define a helper function for reconstructing images\n",
    "    def reconstruct_img(self, x):\n",
    "        # encode image x\n",
    "        z_loc, z_scale = self.encoder(x)\n",
    "        # sample in latent space\n",
    "        z = dist.Normal(z_loc, z_scale).sample()\n",
    "        # decode the image (note we don't sample in image space)\n",
    "        loc_img = self.decoder(z)\n",
    "        return loc_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE()\n",
    "\n",
    "optimizer = Adam({\"lr\": 1.0e-3})\n",
    "svi = SVI(vae.model, vae.guide, optimizer, loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(svi, train_loader, use_cuda=False):\n",
    "    # initialize loss accumulator\n",
    "    epoch_loss = 0.\n",
    "    # do a training epoch over each mini-batch x returned\n",
    "    # by the data loader\n",
    "    for x, _ in train_loader:\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "        # do ELBO gradient and accumulate loss\n",
    "        epoch_loss += svi.step(x)\n",
    "\n",
    "    # return epoch loss\n",
    "    normalizer_train = len(train_loader.dataset)\n",
    "    total_epoch_loss_train = epoch_loss / normalizer_train\n",
    "    return total_epoch_loss_train\n",
    "\n",
    "def evaluate(svi, test_loader, use_cuda=False):\n",
    "    # initialize loss accumulator\n",
    "    test_loss = 0.\n",
    "    # compute the loss over the entire test set\n",
    "    for x, _ in test_loader:\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "        # compute ELBO estimate and accumulate loss\n",
    "        test_loss += svi.evaluate_loss(x)\n",
    "    normalizer_test = len(test_loader.dataset)\n",
    "    total_epoch_loss_test = test_loss / normalizer_test\n",
    "    return total_epoch_loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run options\n",
    "LEARNING_RATE = 1.0e-3\n",
    "USE_CUDA = True\n",
    "\n",
    "# Run only for a single iteration for testing\n",
    "NUM_EPOCHS = 1 if smoke_test else 100\n",
    "TEST_FREQUENCY = 5\n",
    "\n",
    "train_loader, test_loader = setup_data_loaders(batch_size=256, use_cuda=USE_CUDA)\n",
    "\n",
    "# clear param store\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# setup the VAE\n",
    "vae = VAE(use_cuda=USE_CUDA)\n",
    "\n",
    "# setup the optimizer\n",
    "adam_args = {\"lr\": LEARNING_RATE}\n",
    "optimizer = Adam(adam_args)\n",
    "\n",
    "# setup the inference algorithm\n",
    "svi = SVI(vae.model, vae.guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "train_elbo = []\n",
    "test_elbo = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 000] average training loss: 193.3482\n",
      "[epoch 000] average test loss: 158.9939\n",
      "[epoch 001] average training loss: 147.2662\n",
      "[epoch 002] average training loss: 133.0872\n",
      "[epoch 003] average training loss: 124.8822\n",
      "[epoch 004] average training loss: 119.3809\n",
      "[epoch 005] average training loss: 115.8731\n",
      "[epoch 005] average test loss: 113.7173\n",
      "[epoch 006] average training loss: 113.5774\n",
      "[epoch 007] average training loss: 111.9314\n",
      "[epoch 008] average training loss: 110.7310\n",
      "[epoch 009] average training loss: 109.7808\n",
      "[epoch 010] average training loss: 108.9947\n",
      "[epoch 010] average test loss: 107.9958\n",
      "[epoch 011] average training loss: 108.3338\n",
      "[epoch 012] average training loss: 107.7428\n",
      "[epoch 013] average training loss: 107.2396\n",
      "[epoch 014] average training loss: 106.8013\n",
      "[epoch 015] average training loss: 106.5134\n",
      "[epoch 015] average test loss: 106.0608\n",
      "[epoch 016] average training loss: 106.2172\n",
      "[epoch 017] average training loss: 105.9281\n",
      "[epoch 018] average training loss: 105.6829\n",
      "[epoch 019] average training loss: 105.4735\n",
      "[epoch 020] average training loss: 105.2631\n",
      "[epoch 020] average test loss: 104.8010\n",
      "[epoch 021] average training loss: 105.0785\n",
      "[epoch 022] average training loss: 104.8952\n",
      "[epoch 023] average training loss: 104.7646\n",
      "[epoch 024] average training loss: 104.6253\n",
      "[epoch 025] average training loss: 104.4748\n",
      "[epoch 025] average test loss: 104.1398\n",
      "[epoch 026] average training loss: 104.3332\n",
      "[epoch 027] average training loss: 104.2161\n",
      "[epoch 028] average training loss: 104.1176\n",
      "[epoch 029] average training loss: 103.9995\n",
      "[epoch 030] average training loss: 103.8735\n",
      "[epoch 030] average test loss: 103.6368\n",
      "[epoch 031] average training loss: 103.8363\n",
      "[epoch 032] average training loss: 103.7292\n",
      "[epoch 033] average training loss: 103.6286\n",
      "[epoch 034] average training loss: 103.5541\n",
      "[epoch 035] average training loss: 103.4903\n",
      "[epoch 035] average test loss: 103.2997\n",
      "[epoch 036] average training loss: 103.3967\n",
      "[epoch 037] average training loss: 103.3164\n",
      "[epoch 038] average training loss: 103.2271\n",
      "[epoch 039] average training loss: 103.1920\n",
      "[epoch 040] average training loss: 103.1406\n",
      "[epoch 040] average test loss: 102.9833\n",
      "[epoch 041] average training loss: 103.0504\n",
      "[epoch 042] average training loss: 102.9733\n",
      "[epoch 043] average training loss: 102.9398\n",
      "[epoch 044] average training loss: 102.8986\n",
      "[epoch 045] average training loss: 102.8082\n",
      "[epoch 045] average test loss: 102.8615\n",
      "[epoch 046] average training loss: 102.7766\n",
      "[epoch 047] average training loss: 102.7285\n",
      "[epoch 048] average training loss: 102.6395\n",
      "[epoch 049] average training loss: 102.6132\n",
      "[epoch 050] average training loss: 102.5595\n",
      "[epoch 050] average test loss: 102.5170\n",
      "[epoch 051] average training loss: 102.4814\n",
      "[epoch 052] average training loss: 102.4477\n",
      "[epoch 053] average training loss: 102.4226\n",
      "[epoch 054] average training loss: 102.3802\n",
      "[epoch 055] average training loss: 102.3353\n",
      "[epoch 055] average test loss: 102.4445\n",
      "[epoch 056] average training loss: 102.2895\n",
      "[epoch 057] average training loss: 102.2288\n",
      "[epoch 058] average training loss: 102.1903\n",
      "[epoch 059] average training loss: 102.1580\n",
      "[epoch 060] average training loss: 102.0983\n",
      "[epoch 060] average test loss: 102.1370\n",
      "[epoch 061] average training loss: 102.0811\n",
      "[epoch 062] average training loss: 102.0278\n",
      "[epoch 063] average training loss: 102.0022\n",
      "[epoch 064] average training loss: 101.9472\n",
      "[epoch 065] average training loss: 101.9237\n",
      "[epoch 065] average test loss: 102.0988\n",
      "[epoch 066] average training loss: 101.8754\n",
      "[epoch 067] average training loss: 101.8419\n",
      "[epoch 068] average training loss: 101.8020\n",
      "[epoch 069] average training loss: 101.7816\n",
      "[epoch 070] average training loss: 101.7098\n",
      "[epoch 070] average test loss: 101.8465\n",
      "[epoch 071] average training loss: 101.6916\n",
      "[epoch 072] average training loss: 101.6808\n",
      "[epoch 073] average training loss: 101.6538\n",
      "[epoch 074] average training loss: 101.6088\n",
      "[epoch 075] average training loss: 101.5624\n",
      "[epoch 075] average test loss: 101.6747\n",
      "[epoch 076] average training loss: 101.5163\n",
      "[epoch 077] average training loss: 101.5246\n",
      "[epoch 078] average training loss: 101.4691\n",
      "[epoch 079] average training loss: 101.4526\n",
      "[epoch 080] average training loss: 101.4483\n",
      "[epoch 080] average test loss: 101.4871\n",
      "[epoch 081] average training loss: 101.3811\n",
      "[epoch 082] average training loss: 101.3446\n",
      "[epoch 083] average training loss: 101.3389\n",
      "[epoch 084] average training loss: 101.2909\n",
      "[epoch 085] average training loss: 101.2809\n",
      "[epoch 085] average test loss: 101.5749\n",
      "[epoch 086] average training loss: 101.2588\n",
      "[epoch 087] average training loss: 101.2141\n",
      "[epoch 088] average training loss: 101.1886\n",
      "[epoch 089] average training loss: 101.1826\n",
      "[epoch 090] average training loss: 101.1306\n",
      "[epoch 090] average test loss: 101.4263\n",
      "[epoch 091] average training loss: 101.0997\n",
      "[epoch 092] average training loss: 101.0859\n",
      "[epoch 093] average training loss: 101.0458\n",
      "[epoch 094] average training loss: 101.0470\n",
      "[epoch 095] average training loss: 100.9874\n",
      "[epoch 095] average test loss: 101.3117\n",
      "[epoch 096] average training loss: 100.9711\n",
      "[epoch 097] average training loss: 100.9876\n",
      "[epoch 098] average training loss: 100.9441\n",
      "[epoch 099] average training loss: 100.8820\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_epoch_loss_train = train(svi, train_loader, use_cuda=USE_CUDA)\n",
    "    train_elbo.append(-total_epoch_loss_train)\n",
    "    print(\"[epoch %03d] average training loss: %.4f\" % (epoch, total_epoch_loss_train))\n",
    "\n",
    "    if epoch % TEST_FREQUENCY == 0:\n",
    "        # report test diagnostics\n",
    "        total_epoch_loss_test = evaluate(svi, test_loader, use_cuda=USE_CUDA)\n",
    "        test_elbo.append(-total_epoch_loss_test)\n",
    "        print(\"[epoch %03d] average test loss: %.4f\" % (epoch, total_epoch_loss_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f3fa320b400>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_elbo,label=\"train\")\n",
    "plt.legend(loc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
